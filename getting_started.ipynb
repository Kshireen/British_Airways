{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "# for i in range(1, pages + 1):\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ =pd.read_csv('data/BA_reviews.csv')\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.drop(columns=\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\lenono\\anaconda3\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from wordcloud) (1.23.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenono\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[2m\u001b[36m\n",
      "LIBRARIES WERE SUCCESFULLY IMPORTED...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#importing pandas module and numpy module for data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#importing matplotlib and seaborn for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#to remove stopwords from  input data \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Image result for python termcolor\n",
    "#termcolor module is a python module for ANSII Color formatting for output in the terminal\n",
    "from termcolor import colored\n",
    "\n",
    "# to igonore the warning\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# importing wordcloud for creating worldcloud of data\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Using Sklearn library for machine learning models and to vectorize the input data\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "print(colored(\"\\nLIBRARIES WERE SUCCESFULLY IMPORTED...\", color = \"cyan\", attrs = [\"dark\", \"bold\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (579057545.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    df_['reviews'] = regex.sub(\"\", row)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "#data = df_['reviews']\n",
    "\n",
    "regex = re.compile(r\"✅ Trip Verified\")\n",
    "\n",
    "# Remove the \"✅ Trip Verified\" string from each row\n",
    "#for row in data:\n",
    "    df_['reviews'] = regex.sub(\"\", row)\n",
    "    #print(df_['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(dataset):\n",
    "  tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "  \n",
    "  for i in range(dataset.shape[0]): \n",
    "    dataset[\"reviews\"][i] = tokenizer.tokenize(dataset[\"reviews\"][i])\n",
    "  return dataset\n",
    "\n",
    "def remove_stop_words(dataset):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "  for i in range(dataset.shape[0]):\n",
    "    dataset[\"reviews\"][i] = ([token.lower() for token in dataset[\"reviews\"][i] if token not in stop_words])\n",
    "  return dataset\n",
    "\n",
    "def normalize(dataset):\n",
    "  lemmatizer = nltk.stem.WordNetLemmatizer() \n",
    "\n",
    "  for i in range(dataset.shape[0]):\n",
    "    dataset.reviews[i] = \" \".join([lemmatizer.lemmatize(token) for token in dataset.reviews[i]]).strip()\n",
    "  return dataset\n",
    "\n",
    "def remove_garbage(dataset):\n",
    "    garbage = \"~`!@#$%^&*()_-+={[}]|\\:;'<,>.?/<br><url>\"\n",
    "    for i in range(dataset.shape[0]):\n",
    "      dataset.reviews[i] = \"\".join([char for char in dataset.reviews[i] if char not in garbage])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tokenize_data(df)\n",
    "df = remove_stop_words(df)\n",
    "df = normalize(df)\n",
    "df = remove_garbage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
